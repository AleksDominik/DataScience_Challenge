{"cells":[{"metadata":{"toc":true},"cell_type":"markdown","source":"<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n<div class=\"toc\"><ul class=\"toc-item\"></ul></div>"},{"metadata":{"trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nkeras = tf.keras","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tf.random.set_seed(42)\nnp.random.seed(42)\n# bkeras.backend.clear_session()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train=pd.read_csv('/kaggle/input/final-train/train.csv')\nfor country in train.Territory.unique():\n    train.loc[train.Territory==country,\"target\"]=train.loc[train.Territory==country,\"target\"].diff(1)\n#     train.loc[train.Territory==country,\"cases\"]=train.loc[train.Territory==country,\"cases\"].diff(1)\ntrain.dropna(inplace=True)\ntrain.head()\ntrain","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Territory.unique()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's Visualize a the curve of some countries"},{"metadata":{"trusted":true},"cell_type":"code","source":"[train.Territory=='Afghanistan','cases']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def visualize_countries(country):\n    plt.figure(figsize=(13,7))\n#     plt.plot(train.loc[train.Territory==country,'Date'],train.loc[train.Territory==country,'cases'])\n    plt.plot(train.loc[train.Territory==country,'Date'],train.loc[train.Territory==country,'target'])\n#     plt.legend(['cases','target'])\n    \n    plt.xticks(rotation=80)\n    plt.show()\n    \nvisualize_countries(\"Afghanistan\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_countries('China')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_countries(\"United States of America (the)\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"visualize_countries('China')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_country=train.groupby(\"Territory\").sum().reset_index()\nglobal_country.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(10,15))\nfor k in train.Territory.unique():\n    plt.plot(train.loc[train.Territory==k,'Date'],train.loc[train.Territory==k,'target'])\n\nplt.xticks(rotation=80)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_country['target'].describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"global_country['target'].median()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.loc[train.Territory=='China'].shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"enc=LabelEncoder()\nenc.fit(list(train.Territory.unique()))\nenc.transform(['China'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def window_dataset(series, window_size, batch_size=32,\n                   shuffle_buffer=10):\n    dataset = tf.data.Dataset.from_tensor_slices(series)\n    dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n    dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n    dataset = dataset.shuffle(shuffle_buffer)\n    dataset = dataset.map(lambda window: (window[:-1], window[-1]))\n#     dataset = dataset.batch(batch_size).prefetch(1)\n    return dataset\n\ndef preprocessing(country):\n    train_country=train.loc[train.Territory==country,'target'].values\n    scaler=MinMaxScaler(feature_range=(0.,1))\n    train_country=scaler.fit_transform(train_country.reshape(-1,1)).reshape(1,-1)[0]\n       \n    #splitting training data 2/3 for the training part and 1/3 of the part \n    split_data=int(2*train_country.shape[0]/3)\n    series_train=train_country[:split_data]\n    series_validation=train_country[split_data:]\n    print('the length for the specific dataset ',len(train_country))    \n    print('the length of training set is ',series_train.shape,'and the lenght of the validation set is ',series_validation.shape)\n    return [ series_train,series_validation,scaler]\n\ndef define_model(k=100):\n    \n    model = keras.models.Sequential([\n      keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                          input_shape=[None]),\n      keras.layers.LSTM(k, return_sequences=True,\n                         input_shape=[None, 1]),\n      keras.layers.LSTM(k, ),\n#       keras.layers.Dense(100),\n    #       keras.layers.SimpleRNN(1),\n    #       keras.layers.SimpleRNN(100, return_sequences=True),\n    #       keras.layers.SimpleRNN(100),\n            keras.layers.Dense(1),\n#           keras.layers.Lambda(lambda x: x * 500.0)\n        ])\n    return model\ndef define_deeper_model(num_country):\n    inputs_country=tf.keras.layers.Input(shape=(1,))\n    inputs_sequentials=tf.keras.layers.Input(shape=(1,))\n    sequential_model=define_model()(inputs_sequentials)\n    country_layers=tf.keras.layers.Embedding(num_country,32,embeddings_initializer=tf.keras.initializers.GlorotNormal)(inputs_country)\n    add=tf.keras.layers.concatenate([country_layers,sequential_model])\n    dens=tf.keras.layers.Dense(10)(add)\n    output=tf.keras.layers.Dense(1)\n    return tf.keras.Models(inputs=[inputs_sequential,inputs_coutry],outputs=output)\n    \n\ndef sequential_window_dataset(series, window_size,forward_step=7):\n    series = tf.expand_dims(series, axis=-1)\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(  window_size + forward_step, shift=window_size, drop_remainder=True)\n    ds = ds.flat_map(lambda window: window.batch(window_size + forward_step))\n    ds = ds.map(lambda window: (window[:-forward_step], window[window_size:]))\n    return ds\n\nclass ResetStatesCallback(keras.callbacks.Callback):\n    def on_epoch_begin(self, epoch, logs):\n        self.model.reset_states()\n\ndef preprocessing_all_data(window_size,shuffle_buffer,batch_size=100,use_all=False):\n    enc=LabelEncoder()\n    enc.fit(train.Territory.unique())\n    if use_all:\n        split=train.shape[0] # no validation data \n    else:\n        split=int(2*(train.loc[train.Territory=='China','target'].shape[0])/3)\n        \n    series=train.loc[train.Territory=='China','target'].values\n    toscale=train.target.values\n    scaler=MinMaxScaler(feature_range=(0,1))\n    \n    scaler.fit(toscale.reshape(-1,1))\n    series=scaler.transform(series.reshape(-1,1)).reshape(1,-1)[0]\n    \n    dataset_train=window_dataset(series[:split],window_size,enc.transform(['China'])[0])\n    dataset_validation=window_dataset(series[split:],window_size,enc.transform(['China'])[0])\n    \n    for country in train.Territory.unique():\n        series=train.loc[train.Territory==country,'target'].values\n        series=scaler.transform(series.reshape(-1,1)).reshape(1,-1)[0]\n        #create a datset for one country\n        dataset_train_country=window_dataset(series[:split],window_size,enc.transform([country])[0])\n        dataset_validation_country=window_dataset(series[split:],window_size,enc.transform([country])[0])\n        #concatenate in the stack\n        dataset_train=dataset_train.concatenate(dataset_train_country)\n        data_validation=dataset_validation.concatenate(dataset_validation_country)\n        \n    dataset_train=dataset_train.batch(batch_size).prefetch(100)\n    dataset_validation=dataset_validation.batch(batch_size).prefetch(100)\n    return [dataset_train,dataset_validation,scaler]\n\n        \n        \n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#differents type of model\ndef define_model_Rnn():\n    model = keras.models.Sequential([\n      keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                          input_shape=[None]),\n        \n          keras.layers.SimpleRNN(100, return_sequences=True),\n          keras.layers.SimpleRNN(100),\n            keras.layers.Dense(1),\n#           keras.layers.Lambda(lambda x: x * 500.0)\n        ])\n    return model\n\n\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"window_size=25\ndataset_train,dataset_validation,scaler_all_data=preprocessing_all_data(window_size,10,use_all=True)\n#run the code below only once\ntrain_copy=train.copy()\ntrain.target=scaler_all_data.transform(train.target.values.reshape(-1,1)).reshape(1,-1)[0]\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for k in dataset_validation:\n    print('first',len(k))\n    for i in  k:\n        print(len(i))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.max(),train.target.min()","execution_count":null,"outputs":[]},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# window_size = 5\n# train_set = window_dataset(series_train, window_size ,batch_size=10)\n\n# valid_set = window_dataset(series_validation, window_size ,batch_size=10)\ndef decreasing(epochs):\n    lr=1e-5\n    if epochs <10:\n        lr\n    else:\n        lr=lr*(1**(-epochs/10))\n    return lr\nresult={}\n# for k in [20,50,100]:\n#     model = define_model(k)\n\n#     # optimizer = keras.optimizers.SGD(lr=1.5e-6, momentum=0.9)\n#     optimizer=keras.optimizers.Nadam(learning_rate=1e-5)\n#     model.compile(loss=keras.losses.Huber(),\n#                   optimizer=optimizer,\n#                   metrics=[\"mae\"])\n\n\n#     early_stopping = keras.callbacks.EarlyStopping(patience=50)\n#     lr_schedule_decreasing = keras.callbacks.LearningRateScheduler(decreasing)\n#     model_checkpoint = keras.callbacks.ModelCheckpoint(\n#         \"my_checkpoint\", save_best_only=True)\n\n\n\n#     result[k]=model.fit(dataset_train, epochs=50,\n#               validation_data=dataset_validation,\n#               callbacks=[early_stopping, model_checkpoint,lr_schedule_decreasing])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# result={\"10\": result[0],\"50\":result[1],\"100\": result[2]}\nresult","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def plot_history(result,key):\n\n#     plt.boxplot([result[k].history[key] for k in result.keys()],showmeans=True)\n# #     ax.set_title=f'History of {key}'\n#     plt.show()\n    \n# #         plt.xticks=[10,50,100]\n# #     plt.plot(history.history['loss'],label=label)\n\n# plot_history(result,'loss')\n# plot_history(result,'val_loss')\n\n\n    \n#     result[0].history.loss","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"the model with 100 consecutive cellule is better"},{"metadata":{"scrolled":true,"trusted":true},"cell_type":"code","source":"# Optimise learning rate\n# window_size = 5\n# train_set = window_dataset(series_train, window_size)\n# model=define_deeper_model(len(train.Territory.unique()))\nmodel=define_model(100)\nlr_schedule = keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 30))\nreset_states = ResetStatesCallback()\n# optimizer = keras.optimizers.SGD(lr=1e-8, momentum=0.9)\noptimizer=keras.optimizers.Nadam(learning_rate=1e-8)\nmodel.compile(loss=keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\nhistory = model.fit(dataset_train, epochs=500,\n                    callbacks=[lr_schedule])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])","execution_count":null,"outputs":[]},{"metadata":{"scrolled":false,"trusted":true},"cell_type":"code","source":"model = define_model(100)\n\n# optimizer = keras.optimizers.SGD(lr=1.5e-6, momentum=0.9)\noptimizer=keras.optimizers.Nadam(learning_rate=1e-6)\nmodel.compile(loss=keras.losses.Huber(),\n              optimizer=optimizer,\n              metrics=[\"mae\"])\n\n\n# early_stopping = keras.callbacks.EarlyStopping(patience=20)\nlr_schedule_decreasing = keras.callbacks.LearningRateScheduler(decreasing)\nmodel_checkpoint = keras.callbacks.ModelCheckpoint(\n    \"my_checkpoint\", save_best_only=True)\nmodel.fit(dataset_train, epochs=150,\n#           validation_data=dataset_validation,\n          callbacks=[\n#               early_stopping,\n              model_checkpoint,lr_schedule_decreasing])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = keras.models.load_model(\"my_checkpoint\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_forecast(model, series, window_size):\n    ds = tf.data.Dataset.from_tensor_slices(series)\n    ds = ds.window(window_size, shift=1, drop_remainder=True)\n    ds = ds.flat_map(lambda w: w.batch(window_size))\n    ds = ds.batch(32).prefetch(1)\n    forecast = model.predict(ds)\n    return forecast\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot result\ndef plot_series(time, series, format=\"-\", start=0, end=None, label=None):\n    plt.plot(time[start:end], series[start:end], format, label=label)\n    plt.xlabel(\"Time\")\n    plt.ylabel(\"Value\")\n    if label:\n        plt.legend(fontsize=14)\n    plt.grid(True)\n    \n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#lets visualize a long term prediction\n#let considere china\ndef forecast_forward(step_forward,country,train):\n    \"\"\"\n    forecast for a specific country a setp_forward time prediction \n    -step_forward: number of succesive prediction that have to be done\n    - country : name of the country in the train dataset\n    \"\"\"\n\n    country_series=train.loc[train.Territory==country,\"target\"].values\n    predict=country_series\n    for k in range(step_forward):\n\n        predict=np.append(predict,model_forecast(model,predict,window_size)[-1])\n    print(f'the predicted series has a size of {len(predict)}')\n    \n    return predict \n\ndef visualize_forecast(country,step_forward=7,window_size=window_size):\n    \"\"\"\n    it create graph of prediction and actual graph\n    \n    \"\"\"\n    china_series=train.loc[train.Territory==country,\"target\"].values\n    to_predict=china_series[-step_forward:]\n    past_series=china_series[:-step_forward]\n    predict=past_series\n    for k in range(step_forward):\n    #     print(k,predict)\n#         print(len(model_forecast(model,predict,window_size)))\n        predict=np.append(predict,model_forecast(model,predict,window_size)[-1])\n\n    plt.plot(range(len(predict)),predict,'*')\n#     plt.plot(china_series,'o')\n    plt.plot(past_series)\n\n","execution_count":null,"outputs":[]},{"metadata":{"hide_input":true,"trusted":true},"cell_type":"code","source":"forecast_forward(6,'China',train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"original_train=pd.read_csv('/kaggle/input/final-train/train.csv') \n# dont forget to have more element than your input window lenght\nnew_train=train.loc[pd.to_datetime(train.Date)>=pd.to_datetime('2020-03-06')]\nstep_forward=(pd.to_datetime('2020-06-07')-pd.to_datetime(train.Date, format='%m/%d/%y').max()).days\nstep_forward","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"new_train.shape[0]/train.Territory.nunique()+59","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.Date.max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"19646/len(train.Territory.unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import random\ndef cumulative_op(array,first_element):\n    for k in range(len(array)):\n        array[k]=array[k]+first_element\n        first_element=array[k]\n    return array\n\ndef submit(scaler):\n    original_train=pd.read_csv('/kaggle/input/final-train/train.csv')\n    \n    # dont forget to have more element than your input window lenght\n    new_train=train.loc[pd.to_datetime(train.Date)>=pd.to_datetime('2020-03-06')]\n    step_forward=(pd.to_datetime('2020-06-07')-pd.to_datetime(train.Date, format='%m/%d/%y').max()).days\n    dates = pd.date_range(start='2020-03-06', end='2020-06-07', freq='1d')\n    ids = []\n    prediction=np.array([])\n    for country in sorted(train['Territory'].unique()):\n        for d in dates:\n            ids.append(country + ' X ' + d.strftime('%-m/%-d/%y'))\n        prediction=np.append(prediction, cumulative_op( scaler.inverse_transform( forecast_forward(step_forward,country,new_train).reshape(-1,1)).reshape(1,-1)[0],original_train.loc[(pd.to_datetime(original_train.Date)==pd.to_datetime('2020-03-06')) &(original_train.Territory==country) ,'target'].values[0]   ))\n        \n        \n    print(len(prediction),len(ids))\n    ss = pd.DataFrame({\n        'Territory X Date':ids,\n        'target': prediction\n    })\n    \n#     for k in range(ss.shape[0]):\n#         ss['target'].iloc[k]=random.randint(0,1000)\n    ss.loc[ss.target<=0,'target']=0\n    ss.target=ss.target.apply(lambda x:round(x))\n    ss.to_csv('SampleSubmission_mine.csv', index=False)\n    print(ss['target'].sum())\n    ss.head()\nsubmit(scaler_all_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.target.apply(lambda x : round(x) )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# TODO list make a custom model that take in addition a tabular layer for embedding country and the max infected number of people. \n#it will help the model distinguish between country that have less infection .","execution_count":null,"outputs":[]}],"metadata":{"hide_input":false,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"nbformat":4,"nbformat_minor":4}